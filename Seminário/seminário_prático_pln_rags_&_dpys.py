# -*- coding: utf-8 -*-
"""Seminário Prático - PLN - RAGs & DPYs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ea3iJVYctbHxgoEJivGQiMfOIaYgi19T

# **Processamento de Linguagem Natural [2024-Q2]**
Prof. Alexandre Donizeti Alves


---

# **Seminário Prático - How To Build LangChain RAG With DSPy**
**Link para o artigo:** https://medium.com/aiguys/how-to-build-langchain-rag-with-dspy-ce9154fbafaa

# **Introdução teórica**

---

Nesta apresentação, exploraremos como construir e otimizar pipelines de "Retrieval-Augmented Generation" (RAG) integrando as frameworks LangChain, DSPy, e RAGAs. O foco central é aprimorar a capacidade dos Modelos de Linguagem Grande (LLMs) de responder perguntas com maior precisão e relevância, utilizando a integração com DSPy, responsável por automatizar a recuperação de contexto e engenharia de prompts.

## **Principais conceitos apresentados no artigo**

#### **RAGs (Retrieval-Augmented Generation)**
O RAG é uma técnica que melhora a geração de respostas em modelos de linguagem ao integrar dados recuperados de fontes externas, ampliando a capacidade do modelo de acessar informações mais atualizadas e precisas.

#### **DSPy (Retrieval-Augmented Generation)**
O DSPy, por sua vez, é uma estrutura que facilita a programação de fluxos envolvendo esses grandes modelos, automatizando o processo de ajustes e otimizando a geração de textos com base em múltiplas fontes, como dados de busca.

#### **RAGAs (Retrieval-Augmented Generation Assessment)**
RAGAs entra como uma camada adicional de avaliação para monitorar a eficácia das respostas geradas no contexto de recuperação aumentada. Ele avalia o desempenho do sistema de RAG ao verificar se os dados recuperados estão sendo adequadamente usados para aumentar a relevância das respostas geradas, otimizando o modelo continuamente.

**Assim, os três conceitos se integram para formar um ciclo completo de geração, otimização e avaliação de conteúdo em sistemas de PLN, garantindo que as respostas sejam precisas, relevantes e continuamente aprimoradas.**

![alt_text](https://drive.google.com/uc?id=1Q250fMwFuCwLOC4Wh_LvuFIoR96rB8l7)

**Figura retirada do artigo explicando o processo:**

**RAGs -> DSPy -> RAGAs**

# **Implementação do código**

---

Neste tópico, vamos implementar um pipeline de "Retrieval-Augmented Generation" (RAG) utilizando a integração entre LangChain e DSPy. O objetivo é criar um sistema que optimize automaticamente os prompts e melhore a precisão das respostas geradas por Modelos de Linguagem Grande (LLMs).

##### **Instalação das bibliotecas necessárias para o projeto**

**As bibliotecas** `lanchain_openai`, `langchain_community`, `langchain_core` e `langchain_qdrant` fornecem **funcionalidades específicas** para **integração com OpenAI e Qdrant**, permitindo a **construção de pipelines e a utilização de bancos de vetores**.

**`langchain`:** Biblioteca usada para **construir pipelines com LLM** (grandes modelos de linguagem).

**`pydantic`:** Biblioteca para **validação de dados com base em modelos tipados**.

**`dspy_ai`:** Ferramenta que **automatiza a engenharia de prompts, otimizando modelos de linguagem.**

**`qdrant_client`:** Cliente para se **conectar ao banco de dados Qdrant**, que é utilizado para **armazenamento e recuperação de vetores de dados**.

**`pymupdf`:** Biblioteca usada para **manipulação de documentos PDF**.
"""

!pip install -qU langchain==0.2.7
!pip install pydantic==2.8.2
!pip install dspy_ai==2.1.4
!pip install -qU langchain_openai
!pip install -qU langchain_community
!pip install -qU langchain_core
!pip install -qU langchain_qdrant
!pip install -qU qdrant-client
!pip install -qU pymupdf

"""##### **Configuração da chave de acesso da LLM utilizada**

*   Primeiramente, devemos **configurar a chave** para realizarmos requisição para a **API da OpenAI**. Essa etapa é crucial para que possamos seguir para as demais etapas do código.
*   A chave da API é **gerada logo após criar uma conta na plataforma OpenAI** e acessar as configurações de API. Nesse caso, é solicitado para que o usuário **passe a sua chave por parâmetro**, armazenando como **variável de ambiente** para **futuras requisições**.

> *Vale ressaltar que a chave é um **identificador único de cada conta** e, por motivos de **segurança**, após a criação dela, não é possível visualizar ela novamente. Ou seja, **em caso de perda**, **é necessário gerar uma nova** na plataforma*
"""

#Importa o módulo os e getpass
import os
import getpass

#Captura a chave de API do OpenAI e armazena na variável de ambiente
os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key:")

"""##### **Utilizar o PyMuPDFLoader para carregar o documento e dividí-lo**

*   Nesse momento do código, utilizamos a biblioteca do **`PyMuPDFLoader`** para carregar o documento utilizado no artigo.
*   A partir de uma **URL contendo o arquivo**, o `PyMuPDFLoader` consegue **carregar o conteúdo do PDF** e armazenar em uma variável.
*   **Depois** o **conteúdo será separado em "chunks"** (partes menores) a partir do uso do **componente `RecursiveCharacterTextSplitter`** que foi importado da biblioteca **`langchain.text_splitter`**.
"""

#Importa o carregador de PDFs e divisor de texto
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

#Carrega o documento PDF a partir da URL e, sem seguida, carrega o conteúdo do PDF
document_loader = PyMuPDFLoader("https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf")
documents = document_loader.load()

#Define o divisor de texto com tamanho de 1000 caracteres e sobreposição de 200
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

#Realiza a divisão do documento em partes
split_documents = text_splitter.split_documents(documents)

"""##### **Realizar o armazenamento vetorial das informações**

*   A partir dos **chunks obtidos** anteriormente, criamos um **armazenamento vetorial de embeddings** utilizando a biblioteca **`Qdrant`**.
*   Esse **armazenamento vetorial** recupera as partes mais **relevantes do documento**.

> *O que são **"Embeddings"**?*

> **Embeddings** são representações vetoriais de palavras ou frases que capturam seu **significado semântico e relações contextuais**, transformando **texto em números** que podem ser usados para **análise e aprendizado de máquina**.
"""

#Importa o armazenamento vetorial com Qdrant e as embeddings da OpenAI
from langchain_community.vectorstores import Qdrant
from langchain_openai import OpenAIEmbeddings

#Cria embeddings com o modelo "text-embedding-3-small"
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

#Cria o armazenamento vetorial no Qdrant usando os documentos divididos
vectorstore = Qdrant.from_documents(
    split_documents,                  #Documentos a serem inseridos
    embeddings,                       #Embeddings para os documentos
    location=":memory:",              #Armazena na memória (não persistente)
    collection_name="PMarca",         #Nome da coleção
)

#Converte o armazenamento vetorial em um objeto retriever
retriever = vectorstore.as_retriever()

"""##### **Adição de cache no código**

*   A partir da **função `set_llm_cache`** é criado **um sistema de cache**, que irá **otimizar as execuções** do modelo.
*   Essa função será responsável por **armazenar os resultados dos modelos de linguagem em um banco de dados**, de modo a melhorar a **eficiência da LLM** e reduzir a **necessidade de recalcular/reprocessar os resultados**.
"""

#Importa a função para definir cache global e baseado em SQLite
from langchain.globals import set_llm_cache
from langchain_community.cache import SQLiteCache

#Define o cache LLM usando um banco de dados SQLite
set_llm_cache(SQLiteCache(database_path="cache.db"))

"""##### **Função para recuperar o conteúdo do armazenamento vetorial**

*   A **função `retrieve`** é responsável por buscar **documentos relevantes** com base na **pergunta fornecida**.
*   O **retorno** dessa função consiste em uma **lista com o conteúdo das páginas** mais relevantes para a pergunta
"""

#Define a função retrieve para buscar informações
def retrieve(inputs):
    #Retorna o conteúdo da página para cada documento recuperado pela pergunta
    return [doc.page_content for doc in retriever.invoke(inputs["question"])]

"""##### **Integração das funções criadas com a LLM do GPT-4o**

Nessa etapa, nós realizamos a **integração prática da LLM**, **configurando o modelo** GPT-4o-mini para **responder as perguntas** utilizando um pipeline específico.



*   Inicialmente, **definimos o modelo padrão** que iremos utilizar como nossa LLM, **configurando a temperatura como 0** para** minimizar a aleatoriedade** e garantir respostas mais precisas.
*   Em seguida, criamos um **template de prompt** que formata a pergunta para ser respondida no estilo de um tweet.
*   Por fim, **o pipeline processa a pergunta** usando o modelo definido para gerar a resposta, **baseando-se no template de prompt**, e a **função `StrOutputParser`** para **converter a saída em uma string formatada**.
"""

#Importa o modelo de chat da OpenAI, parser de saída, template de prompt e runnable
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

#Inicializa o modelo LLM com GPT-4o-mini e temperatura 0
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

#Cria o template de prompt para gerar uma resposta em formato de tweet em português
prompt = PromptTemplate.from_template(
    "Given {context}, answer the question `{question}` as a tweet. Your response should only contain the tweet.Use  Brazillian portuguese and emojis"
)

#Cria a cadeia RAG simples que passa pelo retrieve, prompt, LLM e parser de saída
naive_rag_chain = (
    RunnablePassthrough.assign(context=retrieve)    #Define 'context' como o retorno da função retrieve
    | prompt                                        #Passa o prompt formatado
    | llm                                           #Passa pelo modelo LLM
    | StrOutputParser()                             #Converte a saída para string
)

"""##### **Integração com o DSPy**

Em seguida, nós **integramos o nosso código com DSPy**, para melhorar a pipeline.

*   A **integração do DSPy** com LangChain funciona **configurando um pipeline** que usa **`RunnablePassthrough`** para definir o contexto com **`retrieve`**, **aplica o** modelo de linguagem **LLM** ao prompt com **`LangChainPredict`**, e **formata a resposta** com **`StrOutputParser`**.
*   O pipeline é **encapsulado em um `LangChainModule`** para modularidade e integração eficiente no sistema DSPy.
"""

#Importa os módulos necessário para integrar com LangChain
from dspy.predict.langchain import LangChainModule, LangChainPredict

#Cria a cadeia de previsão zero-shot
zeroshot_chain = (
    RunnablePassthrough.assign(context=retrieve)  #Define 'context' como o retorno da função retrieve
    | LangChainPredict(prompt, llm)               #Usa LangChainPredict com o prompt e LLM
    | StrOutputParser()                           #Converte a saída para string
)

#Cria um módulo LangChain a partir da cadeia de previsão
zeroshot_chain = LangChainModule(
    zeroshot_chain
)

"""# **Teste final do código**

---

Por fim, realizamos um **teste do modelo desenvolvido**, realizando **perguntas para a LLM** acerca de temas relacionados com o PDF enviado.

##### **Pergunta 1**
"""

question1 = "Qual a melhor parte sobre a Califórnia?"

resposta1 = zeroshot_chain.invoke({"question": question1})
print(resposta1)

"""##### **Pergunta 2**"""

question2 = "Quando eu deveria começar uma startup?"

resposta2 = zeroshot_chain.invoke({"question": question2})
print(resposta2)

"""##### **Pergunta 3**"""

question3 = "Como desenvolver técnicas de crescimento pessoal para ser um bom empreeendedor?"

resposta3 = zeroshot_chain.invoke({"question": question3})
print(resposta3)

"""# **Funcionalidades adicionais**

---

Apesar dessa parte **não estar inserida no artigo**, utilizamos a **ferramenta Streamlit** para criarmos uma **interface para melhor visualização** dos Tweets.

##### **Configurações iniciais do streamlit**
"""

!pip install -qU streamlit
!npm install localtunnel

import pickle

with open('data.pkl', 'wb') as f:
    pickle.dump({
        'questions': [question1, question2, question3],
        'responses': [resposta1, resposta2, resposta3]
    }, f)

"""##### **Passagem dos dados que serão utilizados na interface**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pickle
# import streamlit as st
# 
# logo_url = "https://t.ctcdn.com.br/IztTNlvssS2kgmPP4QPFV5-WKL8=/1080x1080/smart/i489929.jpeg"
# 
# #Carregar perguntas e respostas do arquivo pickle
# with open('data.pkl', 'rb') as f:
#     data = pickle.load(f)
# 
# questions = data['questions']
# responses = data['responses']
# 
# #Lista de URLs para as fotos de perfil (substitua pelos URLs reais)
# profile_urls = [
#     "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTiMCgXOLc4oHKEAxPqJwVrSzoLOKbsnw8KXg&s",
#     "https://revistazelo.com.br/wp-content/uploads/2023/07/X.jpg",
#     "https://i.pinimg.com/236x/ac/0f/41/ac0f419e977af681516e00829c5393ee.jpg",
#     #Adicione mais URLs conforme necessário
# ]
# 
# #Lista de nomes dos usuários (substitua pelos nomes reais)
# user_names = [
#     "Usuário 1",
#     "Usuário 2",
#     "Usuário 3",
#     #Adicione mais nomes conforme necessário
# ]
# 
# #Configurando a interface
# st.image(logo_url, width=80)
# st.title("Generated Tweets")
# 
# for i, (question, response) in enumerate(zip(questions, responses)):
#     with st.container():
#         #Exibir a imagem de perfil e o nome do usuário ao lado
#         if i < len(profile_urls) and i < len(user_names):
#             col1, col2 = st.columns([1, 10])  #Ajuste a proporção conforme necessário
#             with col1:
#                 st.image(profile_urls[i], width=50)
#             with col2:
#                 st.write(user_names[i])
# 
#         st.subheader(f"Q: {question}")
#         st.write(response)
#         st.markdown("---")  #Divisor visual entre tweets
#

"""##### **Execução do ambiente**"""

#Execute o Streamlit em segundo plano e salve logs
!streamlit run app.py &>/content/logs.txt &

#Abra o LocalTunnel para permitir acesso externo
!npx localtunnel --port 8501 & curl ipv4.icanhazip.com
